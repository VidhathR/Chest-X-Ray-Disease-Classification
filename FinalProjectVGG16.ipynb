{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1 / 255.0,\n",
    "                                                       vertical_flip = True,\n",
    "                                                       horizontal_flip = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import VGG16\n",
    "\n",
    "conv_base = VGG16(weights = \"imagenet\",\n",
    "include_top = False,\n",
    "input_shape = (224, 224, 3))\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 251 images belonging to 3 classes.\n",
      "Found 66 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "trainGen = datagen.flow_from_directory(\n",
    "    r'C:\\Users\\vidha\\Health Analytics\\archive\\Covid19-dataset\\train',\n",
    "    target_size = (224,224),\n",
    "    batch_size = 32,\n",
    "    shuffle = True,\n",
    "    class_mode = 'categorical'\n",
    ")\n",
    "\n",
    "datatestgen = keras.preprocessing.image.ImageDataGenerator(rescale=1 / 255.0)\n",
    "                                                       \n",
    "testGen = datatestgen.flow_from_directory(\n",
    "    r'C:\\Users\\vidha\\Health Analytics\\archive\\Covid19-dataset\\test',\n",
    "    target_size = (224,224),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def extractfeatures(sample_count, generator):\n",
    "    i = 0\n",
    "    batch_size = 32\n",
    "    features = np.zeros(shape=(sample_count, 7, 7, 512))\n",
    "    labels = np.zeros(shape=(sample_count, 3))\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = conv_base.predict(inputs_batch)\n",
    "        features[i * batch_size: (i+1) * batch_size] = features_batch\n",
    "        labels[i * batch_size: (i+1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if ((i * batch_size % 1000) == 0 ):\n",
    "            print(\"processed size =\", i * batch_size)\n",
    "        if i * batch_size >= sample_count:\n",
    "            break\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 0s 322ms/step\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = extractfeatures(251, trainGen)\n",
    "test_features, test_labels = extractfeatures(66, testGen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(7, 7, 512)),\n",
    "    keras.layers.Dense(256, activation = 'relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(3, activation = 'softmax'),\n",
    "])\n",
    "classifier.compile(loss = 'categorical_crossentropy', metrics = ['accuracy'], optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 3s 187ms/step - loss: 4.8279 - accuracy: 0.4250 - val_loss: 2.5056 - val_accuracy: 0.3725\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 1s 111ms/step - loss: 1.4766 - accuracy: 0.6100 - val_loss: 0.3374 - val_accuracy: 0.8627\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.5079 - accuracy: 0.7700 - val_loss: 0.2953 - val_accuracy: 0.8824\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.3498 - accuracy: 0.8400 - val_loss: 0.1915 - val_accuracy: 0.9412\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.2271 - accuracy: 0.9150 - val_loss: 0.2125 - val_accuracy: 0.9412\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.1785 - accuracy: 0.9400 - val_loss: 0.1584 - val_accuracy: 0.9412\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1226 - accuracy: 0.9700 - val_loss: 0.1444 - val_accuracy: 0.9216\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0994 - accuracy: 0.9650 - val_loss: 0.1239 - val_accuracy: 0.9216\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 0.0979 - accuracy: 0.9850 - val_loss: 0.1270 - val_accuracy: 0.9412\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1030 - accuracy: 0.9800 - val_loss: 0.1378 - val_accuracy: 0.9412\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0821 - accuracy: 0.9800 - val_loss: 0.1379 - val_accuracy: 0.9412\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.0693 - accuracy: 0.9900 - val_loss: 0.0944 - val_accuracy: 0.9412\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0481 - accuracy: 0.9950 - val_loss: 0.1136 - val_accuracy: 0.9412\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0343 - accuracy: 1.0000 - val_loss: 0.1222 - val_accuracy: 0.9412\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0406 - accuracy: 1.0000 - val_loss: 0.1226 - val_accuracy: 0.9412\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 1s 81ms/step - loss: 0.0345 - accuracy: 0.9900 - val_loss: 0.1153 - val_accuracy: 0.9412\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 1s 82ms/step - loss: 0.0289 - accuracy: 0.9950 - val_loss: 0.1596 - val_accuracy: 0.9216\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0370 - accuracy: 0.9900 - val_loss: 0.1165 - val_accuracy: 0.9412\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.0523 - accuracy: 0.9800 - val_loss: 0.1154 - val_accuracy: 0.9412\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0289 - accuracy: 0.9950 - val_loss: 0.1194 - val_accuracy: 0.9412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25bb26d6bb0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(train_features, train_labels, steps_per_epoch = 251 // 32, epochs = 20, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: VGG16FinalProject.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: VGG16FinalProject.model\\assets\n"
     ]
    }
   ],
   "source": [
    "classifier.save('VGG16FinalProject.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1190 - accuracy: 0.9242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11895649880170822, 0.9242424368858337]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.evaluate(test_features, test_labels, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0276 - accuracy: 0.9880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.027592161670327187, 0.9880478382110596]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.evaluate(train_features, train_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
